# Dynamic Model Discovery Architecture

## The Universal Pattern

This is how **every** OpenAI-compatible REST API works:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           CONNECTION PATTERN                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   1. GET /models                                                        â”‚
â”‚      â””â”€â†’ "What models do you have?"                                     â”‚
â”‚          Returns: [ {id, display_name, pricing, context_length, ...} ]  â”‚
â”‚                                                                         â”‚
â”‚   2. POST /chat/completions (ping test)                                 â”‚
â”‚      â””â”€â†’ "Are you alive?"                                               â”‚
â”‚          Payload: { model: X, messages: [{role:"user", content:"Hi"}] } â”‚
â”‚          Returns: response + latency measurement                        â”‚
â”‚                                                                         â”‚
â”‚   3. Store results:                                                     â”‚
â”‚      â””â”€â†’ Verified models (confirmed working)                            â”‚
â”‚      â””â”€â†’ Response times (speed ranking)                                 â”‚
â”‚      â””â”€â†’ Blocklist (claims serverless but fails)                        â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Why This Matters

### âŒ The Old Way (WRONG - Hardcoded)
```typescript
// DON'T DO THIS
const MODELS = [
  'meta-llama/Llama-3.3-70B-Instruct-Turbo',
  'mistralai/Mixtral-8x7B-Instruct',
  // ... stale list that breaks when provider changes
];
```

### âœ… The New Way (CORRECT - Dynamic Discovery)
```typescript
// DO THIS
const models = await fetch('/models').then(r => r.json());
// Now verify each one actually works...
for (const model of models) {
  const result = await ping(model.id);
  // Store: working, response time, or blocklisted
}
```

## The Key Insight

**Free and paid models respond to the SAME payload.**

If a model appears in `/models` with valid pricing:
- `pricing.input = 0` â†’ FREE (verified by ping test)
- `pricing.input > 0` â†’ PAID (trust the endpoint, don't waste money pinging)
- `pricing = null` â†’ DEDICATED ONLY (skip, won't work serverless)

The contract is simple: If it's listed with pricing, it WILL respond to `/chat/completions`.

## Implementation

### Files Created

| File | Purpose |
|------|---------|
| `src/lib/modelVerification.ts` | Core verification service - ping tests, caching, blocklists |
| `src/lib/useModelVerification.ts` | React hook for easy component integration |
| `src/components/ModelSelector.tsx` | Updated UI with verified models, response times |
| `src/pages/_app.tsx` | Background verification on app load |

### Flow

```
App Load
   â”‚
   â”œâ”€â†’ Check cache (localStorage)
   â”‚      â””â”€â†’ Fresh? Use it. Stale? Continue...
   â”‚
   â”œâ”€â†’ Fetch GET /models (Together.ai, OpenRouter, custom)
   â”‚
   â”œâ”€â†’ Filter to serverless models (pricing !== null)
   â”‚
   â”œâ”€â†’ Ping FREE models only (save money)
   â”‚      â”œâ”€â†’ Success: Store verified + response time
   â”‚      â””â”€â†’ Failure: Add to blocklist
   â”‚
   â”œâ”€â†’ Trust PAID models (if listed, they work)
   â”‚
   â””â”€â†’ Cache results (4 hour TTL)
```

### UI Structure

```
â”Œâ”€ Model Selector â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [ğŸš€ Together.ai] [ğŸŒ OpenRouter] [âš™ï¸ Custom API]         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â­ FAVORITES                                              â”‚
â”‚   â””â”€ Your starred models...                              â”‚
â”‚                                                          â”‚
â”‚ ğŸ†“ FREE MODELS (VERIFIED) - sorted by speed              â”‚
â”‚   â”œâ”€ Model A    âš¡ FAST    â±ï¸ 342ms   ğŸ“ 128K ctx       â”‚
â”‚   â”œâ”€ Model B    ğŸƒ QUICK   â±ï¸ 567ms   ğŸ“ 64K ctx        â”‚
â”‚   â””â”€ Model C    ğŸ¢ SLOW    â±ï¸ 2.1s    ğŸ“ 32K ctx        â”‚
â”‚                                                          â”‚
â”‚ ğŸ’° META                                                   â”‚
â”‚   â”œâ”€ Llama-3.3-70B-Turbo   $0.88/M   â±ï¸ 412ms          â”‚
â”‚   â””â”€ Llama-4-Maverick      $0.27/M   â±ï¸ 298ms          â”‚
â”‚                                                          â”‚
â”‚ ğŸ’° DEEPSEEK                                              â”‚
â”‚   â””â”€ DeepSeek-V3           $1.25/M   â±ï¸ 787ms          â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Custom Endpoints

Any OpenAI-compatible API works the same way:

```typescript
// Ollama (local)
baseUrl: 'http://localhost:11434'

// LM Studio (local)
baseUrl: 'http://localhost:1234'

// Any provider following OpenAI spec
baseUrl: 'https://api.provider.com'
```

The pattern is universal:
- `GET {baseUrl}/v1/models`
- `POST {baseUrl}/v1/chat/completions`

## Background Sync

The verification runs:
1. **On first load** - Full verification if no cache
2. **On login** - Background refresh without blocking UI
3. **Every 4 hours** - Periodic sync while app is open

Cache is stored in localStorage with 4-hour TTL.

## Response Time = Speed Indicator

The ping test measures actual round-trip latency:
- `< 500ms` â†’ âš¡ FAST
- `< 1500ms` â†’ ğŸƒ QUICK
- `< 3000ms` â†’ ğŸ¢ SLOW
- `> 3000ms` â†’ ğŸ¦¥ VERY SLOW

This helps users choose models based on actual performance, not marketing claims.

## The Philosophy

**NEVER HARDCODE MODELS.**

The `/models` endpoint is the source of truth. Always.

If the endpoint goes down or changes, your app should gracefully handle it - not crash because you assumed "Llama-3.3-70B" would always exist.

Dynamic discovery means:
- âœ… New models appear automatically when providers add them
- âœ… Deprecated models disappear without code changes
- âœ… Pricing updates reflect immediately
- âœ… Any compatible endpoint works the same way

---

*This architecture was designed to be universal, resilient, and future-proof.*
---

## Implementation Status: âœ… COMPLETE

**Completed January 10, 2026:**

| Component | Status | Location |
|-----------|--------|----------|
| Core verification service | âœ… | `src/lib/modelVerification.ts` |
| React hook | âœ… | `src/lib/useModelVerification.ts` |
| Model selector UI | âœ… | `src/components/ModelSelector.tsx` |
| Background sync | âœ… | `src/pages/_app.tsx` |
| Hardcoded models | âœ… NUKED | Removed from all files |

**Test Scripts Available:**
- `architecture-review/src/test-any-endpoint.ts` - Universal endpoint tester
- `architecture-review/src/verify-together-models.ts` - Together.ai specific
- `architecture-review/src/verify-openrouter-models.ts` - OpenRouter specific

**Run the universal tester:**
```bash
cd c:\architecture-review\src
npx ts-node test-any-endpoint.ts
```

**Documentation:**
- [MODEL_VERIFICATION_ARCHITECTURE.md](../../architecture-review/docs/MODEL_VERIFICATION_ARCHITECTURE.md) - Full technical reference